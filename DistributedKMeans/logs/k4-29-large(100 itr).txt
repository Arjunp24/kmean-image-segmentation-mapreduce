2020-12-14 00:26:49,602 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/input/params.txt' for reading
2020-12-14 00:26:50,013 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/input/ClusterValues.txt' for reading
2020-12-14 00:26:50,175 INFO org.apache.hadoop.yarn.client.RMProxy (main): Connecting to ResourceManager at ip-172-31-91-69.ec2.internal/172.31.91.69:8032
2020-12-14 00:26:51,867 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat (main): Total input files to process : 1
2020-12-14 00:26:51,878 INFO com.hadoop.compression.lzo.GPLNativeCodeLoader (main): Loaded native gpl library
2020-12-14 00:26:51,880 INFO com.hadoop.compression.lzo.LzoCodec (main): Successfully loaded & initialized native-lzo library [hadoop-lzo rev 29dbe82f1ecfd8384b89f1a32e5e2d9e5dfd7724]
2020-12-14 00:26:52,498 INFO org.apache.hadoop.mapreduce.JobSubmitter (main): number of splits:1
2020-12-14 00:26:52,634 INFO org.apache.hadoop.mapreduce.JobSubmitter (main): Submitting tokens for job: job_1607905506852_0001
2020-12-14 00:26:53,149 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl (main): Submitted application application_1607905506852_0001
2020-12-14 00:26:53,245 INFO org.apache.hadoop.mapreduce.Job (main): The url to track the job: http://ip-172-31-91-69.ec2.internal:20888/proxy/application_1607905506852_0001/
2020-12-14 00:26:53,264 INFO org.apache.hadoop.mapreduce.Job (main): Running job: job_1607905506852_0001
2020-12-14 00:27:05,382 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1607905506852_0001 running in uber mode : false
2020-12-14 00:27:05,383 INFO org.apache.hadoop.mapreduce.Job (main):  map 0% reduce 0%
2020-12-14 00:27:25,494 INFO org.apache.hadoop.mapreduce.Job (main):  map 7% reduce 0%
2020-12-14 00:27:31,517 INFO org.apache.hadoop.mapreduce.Job (main):  map 12% reduce 0%
2020-12-14 00:27:37,540 INFO org.apache.hadoop.mapreduce.Job (main):  map 16% reduce 0%
2020-12-14 00:27:43,562 INFO org.apache.hadoop.mapreduce.Job (main):  map 21% reduce 0%
2020-12-14 00:27:49,585 INFO org.apache.hadoop.mapreduce.Job (main):  map 25% reduce 0%
2020-12-14 00:27:55,606 INFO org.apache.hadoop.mapreduce.Job (main):  map 30% reduce 0%
2020-12-14 00:28:01,628 INFO org.apache.hadoop.mapreduce.Job (main):  map 35% reduce 0%
2020-12-14 00:28:07,649 INFO org.apache.hadoop.mapreduce.Job (main):  map 39% reduce 0%
2020-12-14 00:28:13,670 INFO org.apache.hadoop.mapreduce.Job (main):  map 43% reduce 0%
2020-12-14 00:28:19,689 INFO org.apache.hadoop.mapreduce.Job (main):  map 47% reduce 0%
2020-12-14 00:28:25,711 INFO org.apache.hadoop.mapreduce.Job (main):  map 51% reduce 0%
2020-12-14 00:28:31,730 INFO org.apache.hadoop.mapreduce.Job (main):  map 55% reduce 0%
2020-12-14 00:28:37,748 INFO org.apache.hadoop.mapreduce.Job (main):  map 60% reduce 0%
2020-12-14 00:28:43,765 INFO org.apache.hadoop.mapreduce.Job (main):  map 64% reduce 0%
2020-12-14 00:28:48,789 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 0%
2020-12-14 00:28:58,830 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 4%
2020-12-14 00:28:59,837 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 8%
2020-12-14 00:29:01,845 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 12%
2020-12-14 00:29:02,849 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 38%
2020-12-14 00:29:03,853 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 42%
2020-12-14 00:29:05,859 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 46%
2020-12-14 00:29:06,863 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 96%
2020-12-14 00:29:07,866 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2020-12-14 00:29:07,871 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1607905506852_0001 completed successfully
2020-12-14 00:29:07,980 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 55
	File System Counters
		FILE: Number of bytes read=457385
		FILE: Number of bytes written=6554842
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=104
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=1
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
		S3: Number of bytes read=6990121
		S3: Number of bytes written=14917
		S3: Number of read operations=0
		S3: Number of large read operations=0
		S3: Number of write operations=0
	Job Counters 
		Killed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=26
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4850976
		Total time spent by all reduces in occupied slots (ms)=31077792
		Total time spent by all map tasks (ms)=101062
		Total time spent by all reduce tasks (ms)=323727
		Total vcore-milliseconds taken by all map tasks=101062
		Total vcore-milliseconds taken by all reduce tasks=323727
		Total megabyte-milliseconds taken by all map tasks=155231232
		Total megabyte-milliseconds taken by all reduce tasks=994489344
	Map-Reduce Framework
		Map input records=500000
		Map output records=13000000
		Map output bytes=325592398
		Map output materialized bytes=9876
		Input split bytes=104
		Combine input records=13001618
		Combine output records=2047
		Reduce input groups=429
		Reduce shuffle bytes=9876
		Reduce input records=429
		Reduce output records=429
		Spilled Records=2476
		Shuffled Maps =26
		Failed Shuffles=0
		Merged Map outputs=26
		GC time elapsed (ms)=7964
		CPU time spent (ms)=140910
		Physical memory (bytes) snapshot=11274874880
		Virtual memory (bytes) snapshot=124508676096
		Total committed heap usage (bytes)=11423711232
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=6990121
	File Output Format Counters 
		Bytes Written=14917
2020-12-14 00:29:08,103 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00000' for reading
2020-12-14 00:29:08,158 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00001' for reading
2020-12-14 00:29:08,210 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00002' for reading
2020-12-14 00:29:08,279 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00003' for reading
2020-12-14 00:29:08,365 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00004' for reading
2020-12-14 00:29:08,415 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00005' for reading
2020-12-14 00:29:08,456 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00006' for reading
2020-12-14 00:29:08,504 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00007' for reading
2020-12-14 00:29:08,622 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00008' for reading
2020-12-14 00:29:08,672 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00009' for reading
2020-12-14 00:29:08,731 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00010' for reading
2020-12-14 00:29:08,769 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00011' for reading
2020-12-14 00:29:08,809 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00012' for reading
2020-12-14 00:29:08,851 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00013' for reading
2020-12-14 00:29:08,900 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00014' for reading
2020-12-14 00:29:08,943 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00015' for reading
2020-12-14 00:29:08,985 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00016' for reading
2020-12-14 00:29:09,076 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00017' for reading
2020-12-14 00:29:09,114 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00018' for reading
2020-12-14 00:29:09,151 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00019' for reading
2020-12-14 00:29:09,184 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00020' for reading
2020-12-14 00:29:09,238 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00021' for reading
2020-12-14 00:29:09,287 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00022' for reading
2020-12-14 00:29:09,334 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00023' for reading
2020-12-14 00:29:09,369 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00024' for reading
2020-12-14 00:29:09,414 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00025' for reading
2020-12-14 00:29:10,109 INFO org.apache.hadoop.yarn.client.RMProxy (main): Connecting to ResourceManager at ip-172-31-91-69.ec2.internal/172.31.91.69:8032
2020-12-14 00:29:10,340 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat (main): Total input files to process : 1
2020-12-14 00:29:10,430 INFO org.apache.hadoop.mapreduce.JobSubmitter (main): number of splits:1
2020-12-14 00:29:10,478 INFO org.apache.hadoop.mapreduce.JobSubmitter (main): Submitting tokens for job: job_1607905506852_0002
2020-12-14 00:29:10,528 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl (main): Submitted application application_1607905506852_0002
2020-12-14 00:29:10,535 INFO org.apache.hadoop.mapreduce.Job (main): The url to track the job: http://ip-172-31-91-69.ec2.internal:20888/proxy/application_1607905506852_0002/
2020-12-14 00:29:10,536 INFO org.apache.hadoop.mapreduce.Job (main): Running job: job_1607905506852_0002
2020-12-14 00:29:18,627 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1607905506852_0002 running in uber mode : false
2020-12-14 00:29:18,627 INFO org.apache.hadoop.mapreduce.Job (main):  map 0% reduce 0%
2020-12-14 00:29:35,699 INFO org.apache.hadoop.mapreduce.Job (main):  map 7% reduce 0%
2020-12-14 00:29:41,719 INFO org.apache.hadoop.mapreduce.Job (main):  map 12% reduce 0%
2020-12-14 00:29:47,740 INFO org.apache.hadoop.mapreduce.Job (main):  map 16% reduce 0%
2020-12-14 00:29:53,760 INFO org.apache.hadoop.mapreduce.Job (main):  map 21% reduce 0%
2020-12-14 00:29:59,781 INFO org.apache.hadoop.mapreduce.Job (main):  map 25% reduce 0%
2020-12-14 00:30:05,801 INFO org.apache.hadoop.mapreduce.Job (main):  map 30% reduce 0%
2020-12-14 00:30:11,820 INFO org.apache.hadoop.mapreduce.Job (main):  map 35% reduce 0%
2020-12-14 00:30:17,838 INFO org.apache.hadoop.mapreduce.Job (main):  map 39% reduce 0%
2020-12-14 00:30:23,857 INFO org.apache.hadoop.mapreduce.Job (main):  map 43% reduce 0%
2020-12-14 00:30:29,875 INFO org.apache.hadoop.mapreduce.Job (main):  map 47% reduce 0%
2020-12-14 00:30:35,894 INFO org.apache.hadoop.mapreduce.Job (main):  map 51% reduce 0%
2020-12-14 00:30:41,913 INFO org.apache.hadoop.mapreduce.Job (main):  map 55% reduce 0%
2020-12-14 00:30:47,930 INFO org.apache.hadoop.mapreduce.Job (main):  map 60% reduce 0%
2020-12-14 00:30:53,949 INFO org.apache.hadoop.mapreduce.Job (main):  map 64% reduce 0%
2020-12-14 00:30:57,961 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 0%
2020-12-14 00:31:07,998 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 4%
2020-12-14 00:31:09,001 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 23%
2020-12-14 00:31:10,008 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 31%
2020-12-14 00:31:11,013 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 42%
2020-12-14 00:31:12,017 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 65%
2020-12-14 00:31:13,020 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 92%
2020-12-14 00:31:14,023 INFO org.apache.hadoop.mapreduce.Job (main):  map 100% reduce 100%
2020-12-14 00:31:14,028 INFO org.apache.hadoop.mapreduce.Job (main): Job job_1607905506852_0002 completed successfully
2020-12-14 00:31:14,054 INFO org.apache.hadoop.mapreduce.Job (main): Counters: 55
	File System Counters
		FILE: Number of bytes read=459354
		FILE: Number of bytes written=6563398
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=104
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=1
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
		S3: Number of bytes read=6990121
		S3: Number of bytes written=14918
		S3: Number of read operations=0
		S3: Number of large read operations=0
		S3: Number of write operations=0
	Job Counters 
		Killed reduce tasks=1
		Launched map tasks=1
		Launched reduce tasks=26
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4670688
		Total time spent by all reduces in occupied slots (ms)=23637984
		Total time spent by all map tasks (ms)=97306
		Total time spent by all reduce tasks (ms)=246229
		Total vcore-milliseconds taken by all map tasks=97306
		Total vcore-milliseconds taken by all reduce tasks=246229
		Total megabyte-milliseconds taken by all map tasks=149462016
		Total megabyte-milliseconds taken by all reduce tasks=756415488
	Map-Reduce Framework
		Map input records=500000
		Map output records=13000000
		Map output bytes=325557677
		Map output materialized bytes=9964
		Input split bytes=104
		Combine input records=13001616
		Combine output records=2045
		Reduce input groups=429
		Reduce shuffle bytes=9964
		Reduce input records=429
		Reduce output records=429
		Spilled Records=2474
		Shuffled Maps =26
		Failed Shuffles=0
		Merged Map outputs=26
		GC time elapsed (ms)=8126
		CPU time spent (ms)=137110
		Physical memory (bytes) snapshot=11365052416
		Virtual memory (bytes) snapshot=124130390016
		Total committed heap usage (bytes)=11487674368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=6990121
	File Output Format Counters 
		Bytes Written=14918
2020-12-14 00:31:14,214 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00000' for reading
2020-12-14 00:31:14,260 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00001' for reading
2020-12-14 00:31:14,375 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00002' for reading
2020-12-14 00:31:14,424 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00003' for reading
2020-12-14 00:31:14,470 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00004' for reading
2020-12-14 00:31:14,519 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00005' for reading
2020-12-14 00:31:14,572 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00006' for reading
2020-12-14 00:31:14,622 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00007' for reading
2020-12-14 00:31:14,671 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00008' for reading
2020-12-14 00:31:14,719 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00009' for reading
2020-12-14 00:31:14,760 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00010' for reading
2020-12-14 00:31:14,826 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00011' for reading
2020-12-14 00:31:14,872 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00012' for reading
2020-12-14 00:31:14,925 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00013' for reading
2020-12-14 00:31:14,969 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00014' for reading
2020-12-14 00:31:15,014 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00015' for reading
2020-12-14 00:31:15,058 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00016' for reading
2020-12-14 00:31:15,120 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00017' for reading
2020-12-14 00:31:15,160 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00018' for reading
2020-12-14 00:31:15,226 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00019' for reading
2020-12-14 00:31:15,279 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00020' for reading
2020-12-14 00:31:15,325 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00021' for reading
2020-12-14 00:31:15,375 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00022' for reading
2020-12-14 00:31:15,413 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00023' for reading
2020-12-14 00:31:15,449 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00024' for reading
2020-12-14 00:31:15,500 INFO com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem (main): Opening 's3://arjun-input-data/k4-29_large/part-r-00025' for reading
2020-12-14 00:31:16,194 INFO com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream (main): close closed:false s3://arjun-input-data/k4-29_large/ClusterValues.txt
